{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">EE 461P: Data Science Principles</p>\n",
    "# <p style=\"text-align: center;\">Assignment 1</p>\n",
    "## <p style=\"text-align: center;\">Total points: 90</p>\n",
    "## <p style=\"text-align: center;\">Due: Tues, September 18, submitted via Canvas by 11:59 pm</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UTID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group. If you do change, let the TAs know.\n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Applications of machine learning (10 pts)\n",
    "\n",
    "Read the [article](http://www.datasciencecentral.com/profiles/blogs/20-data-science-systems-used-by-amazon-to-operate-its-business) \"21 data science systems used by Amazon to operate its business\" and pick any two of the data science systems used by Amazon according to this blog.\n",
    "\n",
    "(5 pts each) For each of these two systems you have chosen:\n",
    "\n",
    "What kind of machine learning problem is involved (e.g. classification, regression, clustering, outlier detection,...)? Speculate on what kind of data may be needed and how the results can be useful to the company.\n",
    "\n",
    "\n",
    "## Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2.a: Maximum likelihood estimate (10 pts)\n",
    "\n",
    "Suppose a manager at an internet sales company wants to estimate how fast his salesperson is generating successful leads. Instead of recording the time for each lead, the time taken to generate the next 3 leads are recorded, i.e., there is one recording (denoting the elapsed time) for every 3 consecutive leads. For a specific salesperson, the time intervals recorded are {1,3,1.5,4,2,7,1.2,2,4,3.1} hours. \n",
    "\n",
    "A statistician suggests that if these time intervals are assumed to arise by i.i.d. sampling from the following distribution:\n",
    "$$ p(t) = \\frac{1}{C \\times \\theta^{3}}t^{2}exp^{-\\frac{t}{\\theta}},$$\n",
    "(where C is a normalizing constant). Therefore, if $\\theta$ can be estimated, then he can provide detailed information\n",
    "about the lead generation process, including average rates, variances etc.\n",
    "\n",
    "Find the Maximum Likelihood estimate for $\\theta$ based on the recorded observations.\n",
    "\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume iid observations of lead times and the distribution recommended by the statistician. Let $i$ index the $i^{th}$ observation $t_i$; the likelihood of $n$ observations is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "L(\\theta) = \\prod_{i = 1}^n \\bigg( \\frac{1}{C \\theta^3} t_i^{2}\\exp^{-\\frac{t_i}{\\theta}}\\bigg)\\\\\n",
    "\\text{Taking log on both sides, we have,}\\\\\n",
    "\\log{L(\\theta)} &= \\sum_{i = 1}^n \\bigg( \\frac{1}{C \\theta^3} t_i^{2}\\exp^{-\\frac{t_i}{\\theta}}\\bigg) \\\\\n",
    "&= \\sum_{i = 1}^n \\bigg( 2t_i - \\frac{t_i}{\\theta} -C - 3\\theta \\bigg)\\\\\n",
    "\\text{Taking derivative w.r.t $\\theta$ gives}\\\\\n",
    "\\frac{\\partial \\log{L(\\theta)}}{\\partial \\theta}&= \\frac{\\sum_{i = 1}^n t_i }{\\theta^2} - \\frac{3n}{\\theta} \\\\\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "Notice that first and third terms are not functions of $\\theta$ and the last term is subtracted $n$ times. Solving for $\\theta$ by setting the derivative to 0, we have, \n",
    "\\begin{equation*}\n",
    "\\theta = \\frac{\\sum_{i = 1}^n t_i}{3n}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Substituting $n = 10$ and $t_i$ to the observations provided, we have, $\\theta = 0.96$.\n",
    "\n",
    "Note the distribution used to model lead times is called a gamma distribution. The distribution is a generalization to the exponential distribution (usually used to model inter-arrival times between events modeled as a Poisson process. Sum of 3 inter-arrival times will be Gamma with k=3, which is what I substituted in the (two parameter) Gamma distribution. You can read the details here: https://en.wikipedia.org/wiki/Gamma_distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2.b: Bivariate Normal Distribution (10 pts)\n",
    "\n",
    "Suppose $X$ and $Y$ are two random variables whose joint distribution is Normal (Gaussian), centered at (0,0) and with correlation $\\rho$. (See \"Bivariate Case\" in the Wikipedia entry for \"Multivariate Normal Distribution\" for the equation, or use just use the vector form given in the class notes, with $\\sigma_{12} = \\sigma_{21} = \\rho \\sigma_{x} \\sigma_{y} $). Consider 2 cases:\n",
    "1. $\\sigma_x^2 = 16; \\sigma_y^2 = 9; \\rho = 0$\n",
    "2. $\\sigma_x^2 = 16; \\sigma_y^2 = 9; \\rho = 0.5$\n",
    "\n",
    "a. (4 pts) Obtain contour plots for each of the two distributions using  Python (https://seaborn.pydata.org/generated/seaborn.kdeplot.html). \n",
    "\n",
    "b. (3 pts) View 3-D plots for the two distributions from at least two different viewing perspectives each (http://matplotlib.org/examples/mplot3d/rotate_axes3d_demo.html).\n",
    "\n",
    "c.  (3 pts) Consider the bivariate Normal Distribution given in part (ii). We can rotate this distribution by using the rotation matrix\n",
    "\\begin{equation} \\left[ \\begin{array}{cc} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{array} \\right] \\end{equation}\n",
    "Check Wikipedia for more info on rotation matrices. This is the same as creating two new random variables:\n",
    "\\begin{aligned}\n",
    "U = X \\cos(\\theta) - Y \\sin(\\theta) \\\\\n",
    "V = X \\sin(\\theta) + Y \\cos(\\theta)\n",
    "\\end{aligned}\n",
    "For a rotation of -45 degrees, plot the rotated distribution and fit a bivariate gaussian to this rotated distribution.\n",
    "\n",
    "## Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Multiple Linear Regression in Python (25 pts)\n",
    "\n",
    "Use the following code to import data that can be used for predicted the popularity of online news articles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>log_n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>sqrt_num_hrefs</th>\n",
       "      <th>sqrt_num_self_hrefs</th>\n",
       "      <th>sqrt_num_imgs</th>\n",
       "      <th>sqrt_num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>log_shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>5.662960</td>\n",
       "      <td>0.589474</td>\n",
       "      <td>0.726190</td>\n",
       "      <td>2.236068</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.381944</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.366667</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.626718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>6.025866</td>\n",
       "      <td>0.606796</td>\n",
       "      <td>0.777344</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.980676</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.176128</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6.148468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>4.897840</td>\n",
       "      <td>0.702290</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.477612</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.433812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>5.638355</td>\n",
       "      <td>0.610714</td>\n",
       "      <td>0.766467</td>\n",
       "      <td>2.236068</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.608541</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.278472</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.170120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>6.212606</td>\n",
       "      <td>0.513026</td>\n",
       "      <td>0.662252</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.084168</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.235000</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_tokens_title  log_n_tokens_content  n_unique_tokens  \\\n",
       "0              12              5.662960         0.589474   \n",
       "1              10              6.025866         0.606796   \n",
       "2              11              4.897840         0.702290   \n",
       "3              11              5.638355         0.610714   \n",
       "4              12              6.212606         0.513026   \n",
       "\n",
       "   n_non_stop_unique_tokens  sqrt_num_hrefs  sqrt_num_self_hrefs  \\\n",
       "0                  0.726190        2.236068             1.414214   \n",
       "1                  0.777344        2.828427             0.000000   \n",
       "2                  0.785714        2.000000             1.414214   \n",
       "3                  0.766467        2.236068             2.000000   \n",
       "4                  0.662252        3.741657             1.000000   \n",
       "\n",
       "   sqrt_num_imgs  sqrt_num_videos  average_token_length  num_keywords  \\\n",
       "0            0.0              0.0              4.381944             6   \n",
       "1            1.0              0.0              4.980676             6   \n",
       "2            3.0              0.0              4.477612            10   \n",
       "3            1.0              0.0              4.608541             4   \n",
       "4            1.0              0.0              5.084168            10   \n",
       "\n",
       "      ...      min_positive_polarity  max_positive_polarity  \\\n",
       "0     ...                   0.166667                    1.0   \n",
       "1     ...                   0.100000                    0.5   \n",
       "2     ...                   0.136364                    0.8   \n",
       "3     ...                   0.136364                    0.5   \n",
       "4     ...                   0.100000                    0.5   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0              -0.366667                  -0.80                  -0.10   \n",
       "1              -0.176128                  -0.70                  -0.05   \n",
       "2              -0.150000                  -0.15                  -0.15   \n",
       "3              -0.278472                  -0.50                  -0.05   \n",
       "4              -0.235000                  -0.50                  -0.10   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                 0.0                       0.0                     0.5   \n",
       "1                 0.3                       0.2                     0.2   \n",
       "2                 0.0                       0.0                     0.5   \n",
       "3                 0.0                       0.0                     0.5   \n",
       "4                 0.3                       0.0                     0.2   \n",
       "\n",
       "   abs_title_sentiment_polarity  log_shares  \n",
       "0                           0.0    6.626718  \n",
       "1                           0.2    6.148468  \n",
       "2                           0.0    8.433812  \n",
       "3                           0.0    7.170120  \n",
       "4                           0.0    7.090077  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load database\n",
    "data = pd.read_csv('data_q3.csv')\n",
    "\n",
    "# show the first 5 rows of DB\n",
    "pd.DataFrame.head(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build inputs (features) and output\n",
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1]\n",
    "\n",
    "# standardization (rendering data zero mean and standard deviation 1)\n",
    "X_norm = (X - X.mean()) / X.std()\n",
    "\n",
    "# split DB into training-set and test-set\n",
    "msk = np.random.rand(len(data)) < 0.8\n",
    "X_train = X_norm[msk]\n",
    "y_train = y[msk]\n",
    "\n",
    "X_test = X_norm[~msk]\n",
    "y_test = y[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)  (5 pts) Using ordinary least squares, fit a multiple linear regression (MLR) on all the feature variables using the entire dataset. Report the regression coefficient of each input feature and evaluate the model using mean squared error (MSE).  Example of ordinary least squares in Python is shown in Section 1.1.1 of http://scikit-learn.org/stable/modules/linear_model.html.\n",
    "\n",
    "(b)  (5 pts) Fit an MLR using the training set.  Evaluate the trained model using the training set and the test set, respectively. Compare the two MSE values.\n",
    "\n",
    "(c)  (5 pts) Do you think your MLR model is reasonable for this problem? You may look at the distribution of residuals to provide an informed answer.\n",
    "\n",
    "(d) (5 pts) Identify and list the the three most siginificant features. You might find this link to be helpful: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html.\n",
    "\n",
    "(e) (5 pts) Build a new MLR model using only the three most significant features. Again, use 80% of the data for training and the remaining 20% for testing. Then, Compare the test set MSE values from (b) and (e). Please explain what you observe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Ridge and Lasso Regression (25 points)\n",
    "Use the following codes to import the already-standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load database\n",
    "filename = \"data_q4.csv\"\n",
    "data = pd.read_csv(filename)\n",
    "\n",
    "X = data.values[:,0:8]\n",
    "y = data.values[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of eight features and one target variable.\n",
    "\n",
    "After importing the datasets, use the following code to add additional second-order polynomial and interaction variables to the features matrix.  You should have 44 variables and one target variable. Note that this code adds all combinations of the features with degree less than or equal to two; in practice one may introduce only a few based on domain knowledge or experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = PolynomialFeatures(2, include_bias=False).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you will explore the application of Lasso and Ridge regression using the sklearn package in Python. The following code will split the data into training and test set using [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with **random state 20** and **test_size = 0.33**.  Note: lambda is called alpha in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) (8pts) Use sklearn.linear_model.Lasso and sklearn.linear_model.Ridge classes to do a [5-fold cross validation](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py) using sklearn's [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold). For the sweep of the regularization parameter, we will look at a grid of values ranging from $\\lambda = 10^{10}$ to $\\lambda = 10^{-5}$. In Python, you can consider this range of values as follows:\n",
    "\n",
    "      import numpy as np\n",
    "\n",
    "      alphas =  10**np.linspace(10,-5,100)\n",
    "\n",
    "  Report the best chosen $\\lambda$ based on cross validation. The cross validation should happen on your training data using  average MSE as the scoring metric. \n",
    "\n",
    "b) (8pts) Run ridge and lasso for all of the alphas specified above (on training data), and plot the coefficients learned for each of them - there should be one plot each for lasso and ridge, so a total of two plots; the plots for different features for a method should be on the same plot (e.g. Fig 6.6 of JW). What do you qualitatively observe when value of the regularization parameter is changed? \n",
    "\n",
    "c) (5pts) Run least squares regression, ridge, and lasso on the training data. For ridge and lasso, use only the best regularization parameter. Report the prediction error (MSE) on the test data for each. \n",
    "\n",
    "d) (4pts) Run lasso again with cross validation using [sklearn.linear_model.LassoCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html). Set the cross validation parameters as follows:\n",
    "\n",
    "    LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "\n",
    "Report the best $\\lambda$ based on cross validation. Run lasso on the training data using the best $\\lambda$ and report the coefficeints for 65 variables. What do you observe from these coefficients? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 (10 pts)\n",
    "Please solve problem \\#3.3 on page 174 in Bishop (Chapter 3). The problem has been uploaded to Canvas under 'Files': **CH3 problems from Bishop PRML.pdf**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
